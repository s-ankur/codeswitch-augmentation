{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ3_UfpOxh1O",
        "outputId": "398c75b6-4ba8-4b37-9121-9d75b7df7736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "017GBagkxpK-",
        "outputId": "64c5843a-0709-4ecd-b0f0-4068a7053e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Cs769\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/Cs769\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQSxD8evxpgP",
        "outputId": "437a99c7-ca77-4fc1-def6-a14713b269ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=048fe2abe54b3ce4e1a80bc27121fa75d1c242cbfe35bd91e088a80163c3b2a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: tokenizers, emoji, huggingface-hub, transformers\n",
            "Successfully installed emoji-2.2.0 huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uUL1vIlUyQ6x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import emoji\n",
        "import random\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "_Gch5TwQyUxv",
        "outputId": "ecc45d2b-2239-43d5-e1e3-29df4853c251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-cc4822248948>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_train = pd.read_csv('Dataset/mal_full_offensive_train.csv', sep='\\t', names=['text', 'label','redun'], header=None, error_bad_lines=False)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-401bd2ea-0d2e-4162-8e5a-f942512b42b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>redun</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ആരണ്ട ആരണ്ട തലുണ്ടാകാണാ ആരണ്ട ഞാൻ ആണ്ട ഞാൻ ആണ്...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sushin syam  Shaiju khalid  Midhun manual</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>J A K E S.   B EJ O Y !!!</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16005</th>\n",
              "      <td>കട്ട ലാലേട്ടൻ ഫാൻസിന് ദൈവത്തെ ഓർത്ത് അമിത പ്രത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16006</th>\n",
              "      <td>ente mammookka ningal puliyalla oru simhama......</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16007</th>\n",
              "      <td>Lucifer mass dialogues Ellam onnu comment chey...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16008</th>\n",
              "      <td>Like from Madurai (Tamil nadu) ....</td>\n",
              "      <td>not-malayalam</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16009</th>\n",
              "      <td>അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16010 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-401bd2ea-0d2e-4162-8e5a-f942512b42b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-401bd2ea-0d2e-4162-8e5a-f942512b42b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-401bd2ea-0d2e-4162-8e5a-f942512b42b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text          label  redun\n",
              "0      പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...  Not_offensive    NaN\n",
              "1      ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...  Not_offensive    NaN\n",
              "2      ആരണ്ട ആരണ്ട തലുണ്ടാകാണാ ആരണ്ട ഞാൻ ആണ്ട ഞാൻ ആണ്...  Not_offensive    NaN\n",
              "3              Sushin syam  Shaiju khalid  Midhun manual  Not_offensive    NaN\n",
              "4                              J A K E S.   B EJ O Y !!!  Not_offensive    NaN\n",
              "...                                                  ...            ...    ...\n",
              "16005  കട്ട ലാലേട്ടൻ ഫാൻസിന് ദൈവത്തെ ഓർത്ത് അമിത പ്രത...  Not_offensive    NaN\n",
              "16006  ente mammookka ningal puliyalla oru simhama......  Not_offensive    NaN\n",
              "16007  Lucifer mass dialogues Ellam onnu comment chey...  Not_offensive    NaN\n",
              "16008                Like from Madurai (Tamil nadu) ....  not-malayalam    NaN\n",
              "16009  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...  Not_offensive    NaN\n",
              "\n",
              "[16010 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.read_csv('Dataset/mal_full_offensive_train.csv', sep='\\t', names=['text', 'label','redun'], header=None, error_bad_lines=False)\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "A8W1J3hxzSTe",
        "outputId": "1004ee6c-c048-4379-a5e5-11bc63e42073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16010\n",
            "16010\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-829ca7a8-8f13-4a71-aa6b-23fcc57ff6a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>redun</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ആരണ്ട ആരണ്ട തലുണ്ടാകാണാ ആരണ്ട ഞാൻ ആണ്ട ഞാൻ ആണ്...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sushin syam  Shaiju khalid  Midhun manual</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>J A K E S.   B EJ O Y !!!</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16005</th>\n",
              "      <td>കട്ട ലാലേട്ടൻ ഫാൻസിന് ദൈവത്തെ ഓർത്ത് അമിത പ്രത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16006</th>\n",
              "      <td>ente mammookka ningal puliyalla oru simhama......</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16007</th>\n",
              "      <td>Lucifer mass dialogues Ellam onnu comment chey...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16008</th>\n",
              "      <td>Like from Madurai (Tamil nadu) ....</td>\n",
              "      <td>not-malayalam</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16009</th>\n",
              "      <td>അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16010 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-829ca7a8-8f13-4a71-aa6b-23fcc57ff6a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-829ca7a8-8f13-4a71-aa6b-23fcc57ff6a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-829ca7a8-8f13-4a71-aa6b-23fcc57ff6a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text          label  redun\n",
              "0      പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...  Not_offensive    NaN\n",
              "1      ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...  Not_offensive    NaN\n",
              "2      ആരണ്ട ആരണ്ട തലുണ്ടാകാണാ ആരണ്ട ഞാൻ ആണ്ട ഞാൻ ആണ്...  Not_offensive    NaN\n",
              "3              Sushin syam  Shaiju khalid  Midhun manual  Not_offensive    NaN\n",
              "4                              J A K E S.   B EJ O Y !!!  Not_offensive    NaN\n",
              "...                                                  ...            ...    ...\n",
              "16005  കട്ട ലാലേട്ടൻ ഫാൻസിന് ദൈവത്തെ ഓർത്ത് അമിത പ്രത...  Not_offensive    NaN\n",
              "16006  ente mammookka ningal puliyalla oru simhama......  Not_offensive    NaN\n",
              "16007  Lucifer mass dialogues Ellam onnu comment chey...  Not_offensive    NaN\n",
              "16008                Like from Madurai (Tamil nadu) ....  not-malayalam    NaN\n",
              "16009  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...  Not_offensive    NaN\n",
              "\n",
              "[16010 rows x 3 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(df_train))\n",
        "#df_train = df_train[df_train['label'] != 'not-malayalam']\n",
        "print(len(df_train))\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7uY_UpCzWeb",
        "outputId": "602fd706-4b57-4d9c-ed69-354b76111fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334\n"
          ]
        }
      ],
      "source": [
        "concatenated = df_train['text'].str.cat(sep=' ')\n",
        "characters_no_emoji = {c for c in set(concatenated) if not emoji.is_emoji(c)}\n",
        "vocabulary = list(characters_no_emoji)\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdB3Q-UAz_UL",
        "outputId": "a686b8bf-60b2-4728-9d4f-93fa9cfeff03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "334\n"
          ]
        }
      ],
      "source": [
        "clean_vocabulary = (((set(vocabulary) - set(string.punctuation)) - set(string.whitespace)) - { '²','ا','ه', '‘','ـ','€','£','\\U000e0065','\\xa0','\\u2069','\\u200b','\\u200c','\\u200d','’','“','”','ഃ','₹','℅','…','\\U000e007f', '⃣', '\\U000e0067','।','→','´','•','\\u2066','\\U000fe4eb','\\U000e006e','\\U000e0062'})\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n3a8nY70ChT",
        "outputId": "c343bfc1-a010-45f3-a2b8-c2abb8e5156e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ವ', '2', 'ق', 'ா', '൨', 'ആ', '൪', '4', 'ள', 'ك', 'ഞ', 'é', 'ഷ', '൦', 'ಲ', 'இ', '3', 'ಯ', 'i', 'ഊ', 'ண', 'أ', 'ൺ', 'റ', 'o', 'ಾ', 'డ', 'ع', 'ன', 'ص', 'ప', 'ò', 'ള', 'ഹ', 'ಗ', 'V', 'న', 'p', 'జ', 'క', 'െ', 'ó', '°', 'ఎ', 'ح', '്', 'ಮ', 'Â', 'Ł', 'ന', 'G', 'Í', 'ř', '०', 'ൻ', 'త', 'ీ', 'സ', 'ᴍ', 'ే', 'ț', 'n', 'ങ', 'ர', '్', 'േ', 'F', 'ഔ', 'ല', 'ె', 'ര', 'Z', 'ഏ', 'ా', 'Ğ', 'k', 'ṭ', 'Q', 'ർ', 'ி', 'Ę', 'ത', 'q', 'ು', 'م', 'ಕ', 'A', 'x', '8', 'ر', 'ബ', 'r', 'E', 'ம', 'അ', 'ب', 'ం', 'e', 'ʀ', 'ో', 'v', 'ഴ', 'y', 'ఉ', 'د', 'Į', 'യ', 'ജ', '್', 'అ', 'గ', 'ూ', 'l', 'ഭ', 'z', 'س', 'ഠ', 'ļ', 'Ī', 'ధ', 'ട', 'ు', 'ೋ', 'ு', 'N', 'ᴅ', 'ഖ', 'U', 'ɴ', 'c', 'b', 'ಿ', 'Ã', 'ഢ', 'ʜ', 'చ', 'W', 'ర', 'ل', 'ై', 'ā', 'എ', 'm', 'ì', 'ً', '¥', 'ʟ', 'à', 'D', 'ᕦ', 'ஜ', 's', 'ொ', 'ش', 'T', 'ൌ', 'ఈ', 'ض', 'Y', 'و', 'ᴇ', 'ū', 'ഉ', 'K', 'ᴛ', 'ഘ', 'Ì', 'M', 'ൗ', 'ൈ', 'ನ', '5', 'Ť', 'P', '0', 'ഇ', 'u', 'Ś', 'ز', '6', 'B', 'ರ', 'f', 'స', 'ു', 'Å', 'ം', 'L', 'ట', 'ṣ', 'ಟ', 'Ľ', 'ൾ', 'w', 'Ĺ', 'വ', 'ல', 'ൃ', 'g', 'ഒ', 'ധ', 'య', 'È', 't', 'ோ', 'ಂ', 'మ', 'ഈ', '்', 'వ', 'ഥ', 'C', 'ണ', 'ಳ', 'ᴏ', 'Ř', 'Ć', 'ോ', 'ീ', 'ć', 'ಅ', 'ن', '1', 'a', 'O', 'I', 'É', 'ى', 'ద', 'ý', 'ட', 'ക', 'ശ', 'd', 'മ', 'ಡ', 'க', 'ച', 'ల', 'ñ', 'శ', 'ൂ', 'ೆ', 'ి', 'ே', 'ൊ', 'ّ', 'ഐ', 'ി', 'ഡ', 'Ń', 'j', 'ഗ', 'J', 'வ', 'ಭ', '9', 'ദ', 'ந', 'த', 'خ', 'ഓ', 'ي', 'ഛ', 'S', 'ت', 'ف', 'బ', 'Ç', 'ൽ', 'ದ', 'X', 'ഫ', 'ٍ', 'ಶ', 'ê', 'h', 'Ė', 'ಹ', 'R', 'H', '7', '൯', 'പ', 'ാ']\n"
          ]
        }
      ],
      "source": [
        "vocabulary=list(clean_vocabulary)\n",
        "print(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Fkd43IZ0GVb"
      },
      "outputs": [],
      "source": [
        "def augment_sentence_swapping(sentence, p=0.5):\n",
        "    # Split sentence into words and process each word separately\n",
        "    words = sentence.split()\n",
        "    for i, word in enumerate(words):\n",
        "        # Randomly decide whether to swap one adjacent character\n",
        "        if random.random() < p:\n",
        "            # Check if the word has at least two non-space/non-emoji characters\n",
        "            chars = [c for c in word if c not in string.whitespace and not emoji.is_emoji(c)]\n",
        "            if len(chars) >= 2:\n",
        "                # Choose a random non-space/non-emoji character position to swap\n",
        "                pos = random.randint(0, len(chars)-2)\n",
        "                # Swap the chosen character with the next character\n",
        "                new_word = ''\n",
        "                j = 0\n",
        "                while j < len(word):\n",
        "                    if word[j] not in string.whitespace and not emoji.is_emoji(word[j]):\n",
        "                        if j == pos:\n",
        "                            new_word += word[j+1] + word[j]\n",
        "                            j += 2\n",
        "                        elif j == pos + 1:\n",
        "                            j += 1\n",
        "                        else:\n",
        "                            new_word += word[j]\n",
        "                            j += 1\n",
        "                    else:\n",
        "                        new_word += word[j]\n",
        "                        j += 1\n",
        "                words[i] = new_word\n",
        "    # Join the processed words back into a sentence\n",
        "    augmented_sentence = ' '.join(words)\n",
        "    return augmented_sentence\n",
        "\n",
        "def augment_sentence_substitute(sentence, char_list, p=0.5):\n",
        "    # Split sentence into words and process each word separately\n",
        "    words = sentence.split()\n",
        "    for i, word in enumerate(words):\n",
        "        # Randomly decide whether to substitute a character\n",
        "        if random.random() < p:\n",
        "            # Check if the word has at least one non-space/non-emoji character\n",
        "            chars = [c for c in word if c not in string.whitespace and not emoji.is_emoji(c)]\n",
        "            if len(chars) >= 1:\n",
        "                # Choose a random non-space/non-emoji character position to substitute\n",
        "                pos = random.randint(0, len(chars)-1)\n",
        "                # Choose a random character from the given list to substitute with\n",
        "                new_char = random.choice(char_list)\n",
        "                # Substitute the chosen character with the new character\n",
        "                new_word = ''\n",
        "                j = 0\n",
        "                while j < len(word):\n",
        "                    if word[j] not in string.whitespace and not emoji.is_emoji(word[j]):\n",
        "                        if j == pos:\n",
        "                            new_word += new_char\n",
        "                            j += 1\n",
        "                        else:\n",
        "                            new_word += word[j]\n",
        "                            j += 1\n",
        "                    else:\n",
        "                        new_word += word[j]\n",
        "                        j += 1\n",
        "                words[i] = new_word\n",
        "    # Join the processed words back into a sentence\n",
        "    augmented_sentence = ' '.join(words)\n",
        "    return augmented_sentence\n",
        "\n",
        "\n",
        "def augment_sentence_deleteChar(sentence, p=0.5):\n",
        "    # Split sentence into words and process each word separately\n",
        "    words = sentence.split()\n",
        "    for i, word in enumerate(words):\n",
        "        # Randomly decide whether to delete a character\n",
        "        if random.random() < p:\n",
        "            # Check if the word has at least two characters\n",
        "            if len(word) >= 2:\n",
        "                # Choose a random character position to delete\n",
        "                pos = random.randint(0, len(word)-1)\n",
        "                # Delete the chosen character\n",
        "                new_word = word[:pos] + word[pos+1:]\n",
        "                words[i] = new_word\n",
        "    # Join the processed words back into a sentence\n",
        "    augmented_sentence = ' '.join(words)\n",
        "    return augmented_sentence\n",
        "\n",
        "\n",
        "def augment_sentence_insert(sentence, char_list, p=0.5):\n",
        "    # Split sentence into words and process each word separately\n",
        "    words = sentence.split()\n",
        "    for i, word in enumerate(words):\n",
        "        # Randomly decide whether to insert a character\n",
        "        if random.random() < p:\n",
        "            # Choose a random character from the given list\n",
        "            char = random.choice(char_list)\n",
        "            # Choose a random position to insert the character\n",
        "            pos = random.randint(0, len(word))\n",
        "            # Insert the chosen character at the chosen position\n",
        "            new_word = word[:pos] + char + word[pos:]\n",
        "            words[i] = new_word\n",
        "    # Join the processed words back into a sentence\n",
        "    augmented_sentence = ' '.join(words)\n",
        "    return augmented_sentence\n",
        "\n",
        "def augment_sentence(sentence, char_list, p=0.5):\n",
        "    # Define the four operations as lambda functions\n",
        "    insert_op = lambda word: augment_sentence_insert(word, char_list, p)\n",
        "    delete_op = lambda word: augment_sentence_deleteChar(word)\n",
        "    substitute_op = lambda word: augment_sentence_substitute(word, char_list, p)\n",
        "    swap_op = lambda word: augment_sentence_swapping(word)\n",
        "    # Define a list of the four operations\n",
        "    ops = [insert_op, delete_op, substitute_op, swap_op]\n",
        "    # Split sentence into words and process each word separately\n",
        "    words = sentence.split()\n",
        "    for i, word in enumerate(words):\n",
        "        # Choose a random operation and apply it to the word\n",
        "        random_op = random.choice(ops)\n",
        "        words[i] = random_op(word)\n",
        "    # Join the processed words back into a sentence\n",
        "    augmented_sentence = ' '.join(words)\n",
        "    return augmented_sentence\n",
        "\n",
        "def generate_augmented_sentences(row):\n",
        "    sentences = [row['text']]\n",
        "    labels = [row['label']]\n",
        "    for i in range(3):\n",
        "        sentences.append(augment_sentence(row['text'], vocabulary))\n",
        "        labels.append(row['label'])\n",
        "    return pd.concat([pd.Series(sentences), pd.Series(labels)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Sr-moVSI1Nrx",
        "outputId": "51256b82-26e8-46a7-dede-173782808b2b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hഥello'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "augment_sentence_insert(\"Hello\", vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rWgKVQdQ1SAc"
      },
      "outputs": [],
      "source": [
        "augmented_data = []\n",
        "for i, row in df_train.iterrows():\n",
        "  augmented_data.append({'text': row['text'], 'label': row['label']})\n",
        "  for _ in range(3):\n",
        "    augmented_data.append({'text': augment_sentence(row['text'], vocabulary), 'label': row['label']})\n",
        "augmented_df = pd.DataFrame(augmented_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSsSediN1lz0",
        "outputId": "84ce26bd-f742-4250-9c9e-07f147226862"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64040"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(augmented_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qNKSX9Vi1r_R"
      },
      "outputs": [],
      "source": [
        "augmented_df.to_csv('augmented_data_mal.csv', sep='\\t', index=False, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gguw0_Kv2Y2s",
        "outputId": "9b1cd76d-47ff-42df-ea8a-72c79ad0d5f7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-72383497-d39f-4b55-9d5e-7bd0baf035e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>പലദഥേശം. പല ാഭഷ ഒരdേ ഒരു രാఉാവ് അല്ലാതെ സ്വന്ത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6ലദേശം. സപല ഭంഷ ഒരേ ഒരു രാജാവ് അല్്ലാതെ സ്വന്ത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>പലദേശം. പല ഭഷാ ഒരേ ഒരു രാജحാവ് അല്sലാതെ സ്വന്ത...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64035</th>\n",
              "      <td>Like rfom Madurai (Tamli nadu) ....</td>\n",
              "      <td>not-malayalam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64036</th>\n",
              "      <td>അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64037</th>\n",
              "      <td>അടിമകൾ യആി ജീവിച്ചു മാറിക്കയല്ല ചാവേബറായി ചാവാ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64038</th>\n",
              "      <td>അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64039</th>\n",
              "      <td>അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായధി ചാവാ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64040 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72383497-d39f-4b55-9d5e-7bd0baf035e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72383497-d39f-4b55-9d5e-7bd0baf035e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72383497-d39f-4b55-9d5e-7bd0baf035e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text          label\n",
              "0      പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...  Not_offensive\n",
              "1      പലദഥേശം. പല ാഭഷ ഒരdേ ഒരു രാఉാവ് അല്ലാതെ സ്വന്ത...  Not_offensive\n",
              "2      6ലദേശം. സപല ഭంഷ ഒരേ ഒരു രാജാവ് അല్്ലാതെ സ്വന്ത...  Not_offensive\n",
              "3      പലദേശം. പല ഭഷാ ഒരേ ഒരു രാജحാവ് അല്sലാതെ സ്വന്ത...  Not_offensive\n",
              "4      ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...  Not_offensive\n",
              "...                                                  ...            ...\n",
              "64035                Like rfom Madurai (Tamli nadu) ....  not-malayalam\n",
              "64036  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...  Not_offensive\n",
              "64037  അടിമകൾ യആി ജീവിച്ചു മാറിക്കയല്ല ചാവേബറായി ചാവാ...  Not_offensive\n",
              "64038  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...  Not_offensive\n",
              "64039  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായధി ചാവാ...  Not_offensive\n",
              "\n",
              "[64040 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "augmented_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "EzDBPg4A2al_",
        "outputId": "9f34767c-4048-4636-9843-ccb7dd853cd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-65cb88f70f9b>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df_val = pd.read_csv('Dataset/mal_full_offensive_dev.csv', sep='\\t', names=['text', 'label','redun'], header=None, error_bad_lines=False)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4ee6ab38-2587-4229-9a7d-03304e2ca31a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>redun</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gopichettante BGM um mammookayum ishtapedunnav...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ഇത് ഒരു പെണ്ണ് തന്നെ ആണോ direct ചെയ്യുന്നത്  p...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>പൃഥ്വിരാജ് സുരാജേട്ടൻ ലാലും അലക്സ്.. Lal jr. ന...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>പോകരുത് മക്കളെ പോക്ക .......... നൻ കണ്ട് എന്റ്...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>avan Varum ente Makan Madhura Raja....</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1994</th>\n",
              "      <td>Onnam ഇട്ടി മണി &amp; brathors ഡേ വിന്നർ ലാലേട്ടൻ ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>Manju chechi undel hero vendaa. Chechi vere le...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>Ith kollaaaam....kanda ooodayp trailer ne kalu...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>Ufffff.  Pwoli lalattaa... Katta waiting</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>trailer kollam but oru tamil movie remake anu ...</td>\n",
              "      <td>Not_offensive</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1999 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ee6ab38-2587-4229-9a7d-03304e2ca31a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ee6ab38-2587-4229-9a7d-03304e2ca31a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ee6ab38-2587-4229-9a7d-03304e2ca31a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text          label  redun\n",
              "0     Gopichettante BGM um mammookayum ishtapedunnav...  Not_offensive    NaN\n",
              "1     ഇത് ഒരു പെണ്ണ് തന്നെ ആണോ direct ചെയ്യുന്നത്  p...  Not_offensive    NaN\n",
              "2     പൃഥ്വിരാജ് സുരാജേട്ടൻ ലാലും അലക്സ്.. Lal jr. ന...  Not_offensive    NaN\n",
              "3     പോകരുത് മക്കളെ പോക്ക .......... നൻ കണ്ട് എന്റ്...  Not_offensive    NaN\n",
              "4                avan Varum ente Makan Madhura Raja....  Not_offensive    NaN\n",
              "...                                                 ...            ...    ...\n",
              "1994  Onnam ഇട്ടി മണി & brathors ഡേ വിന്നർ ലാലേട്ടൻ ...  Not_offensive    NaN\n",
              "1995  Manju chechi undel hero vendaa. Chechi vere le...  Not_offensive    NaN\n",
              "1996  Ith kollaaaam....kanda ooodayp trailer ne kalu...  Not_offensive    NaN\n",
              "1997           Ufffff.  Pwoli lalattaa... Katta waiting  Not_offensive    NaN\n",
              "1998  trailer kollam but oru tamil movie remake anu ...  Not_offensive    NaN\n",
              "\n",
              "[1999 rows x 3 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_val = pd.read_csv('Dataset/mal_full_offensive_dev.csv', sep='\\t', names=['text', 'label','redun'], header=None, error_bad_lines=False)\n",
        "df_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzUGlgF82re8",
        "outputId": "30bd1320-ebb3-4e68-e1dd-21fe825f89fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-03-29 15:08:31.201588: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-29 15:08:31.201692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-29 15:08:31.201712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
            "  jax.tree_util.register_keypaths(\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "2023-03-29 15:08:37.694123: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "/content/drive/MyDrive/Cs769/code/StandAloneTrainingMalAugmented.py:208: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  trainData = pd.read_csv('augmented_data_mal.csv', sep='\\t', names=['text', 'label','redun'], header=None, error_bad_lines=False)\n",
            "/content/drive/MyDrive/Cs769/code/StandAloneTrainingMalAugmented.py:209: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  valData =  pd.read_csv(\"Dataset/mal_full_offensive_dev.csv\", sep='\\t', names=['text', 'label','redun'], header=None, encoding='utf-8', error_bad_lines=False)\n",
            "/content/drive/MyDrive/Cs769/code/StandAloneTrainingMalAugmented.py:210: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  testData =  pd.read_csv(\"Dataset/mal_full_offensive_test.csv\", sep='\\t', names=['text', 'label','redun'], header=None,encoding='utf-8', error_bad_lines=False)\n",
            "                                                    text  ...  redun\n",
            "0      പലദേശം. പല ഭാഷ ഒരേ ഒരു രാജാവ്  അല്ലാതെ  സ്വന്ത...  ...    NaN\n",
            "1      പലദഥേശം. പല ാഭഷ ഒരdേ ഒരു രാఉാവ് അല്ലാതെ സ്വന്ത...  ...    NaN\n",
            "2      6ലദേശം. സപല ഭంഷ ഒരേ ഒരു രാജാവ് അല్്ലാതെ സ്വന്ത...  ...    NaN\n",
            "3      പലദേശം. പല ഭഷാ ഒരേ ഒരു രാജحാവ് അല്sലാതെ സ്വന്ത...  ...    NaN\n",
            "4      ഈ ഓണം ഏട്ടനും പിള്ളേർക്ക് ഉള്ളതാണ് എന്ന് ഉള്ളവ...  ...    NaN\n",
            "...                                                  ...  ...    ...\n",
            "64031  Lucifer mass dialogues Ellam onأnu commనent ch...  ...    NaN\n",
            "64036  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...  ...    NaN\n",
            "64037  അടിമകൾ യആി ജീവിച്ചു മാറിക്കയല്ല ചാവേബറായി ചാവാ...  ...    NaN\n",
            "64038  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായി ചാവാറ...  ...    NaN\n",
            "64039  അടിമകൾ ആയി ജീവിച്ചു മാറിക്കയല്ല ചാവേറായధി ചാവാ...  ...    NaN\n",
            "\n",
            "[58892 rows x 3 columns]\n",
            "IS NAN train\n",
            "False\n",
            "IS NAN valData\n",
            "False\n",
            "IS NAN testData\n",
            "False\n",
            "IS NAN train\n",
            "False\n",
            "IS NAN valData\n",
            "False\n",
            "IS NAN testData\n",
            "False\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "100% 58892/58892 [00:20<00:00, 2860.29it/s]\n",
            "100% 58892/58892 [00:02<00:00, 29225.70it/s]\n",
            "100% 58892/58892 [00:04<00:00, 12737.57it/s]\n",
            "[0 0 0 ... 0 0 0]\n",
            "torch.int64 tensor([0, 0, 0,  ..., 0, 0, 0])\n",
            "100% 1836/1836 [00:00<00:00, 3358.54it/s]\n",
            "100% 1836/1836 [00:00<00:00, 52765.77it/s]\n",
            "100% 1836/1836 [00:00<00:00, 17938.36it/s]\n",
            "[0 0 0 ... 0 0 0]\n",
            "torch.int64 tensor([0, 0, 0,  ..., 0, 0, 0])\n",
            "100% 1844/1844 [00:00<00:00, 3275.25it/s]\n",
            "100% 1844/1844 [00:00<00:00, 52905.06it/s]\n",
            "100% 1844/1844 [00:00<00:00, 19510.70it/s]\n",
            "[0 0 0 ... 0 0 0]\n",
            "torch.int64 tensor([0, 0, 0,  ..., 0, 0, 0])\n",
            "[0.03871493581471168, 0.9612850641852884] HW\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.27.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/800c34f3d5aa174fe531f560b44b8d14592225b7/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing SC_weighted_BERT: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing SC_weighted_BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SC_weighted_BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SC_weighted_BERT were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'loss_fct.weight', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:28.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:11.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:40.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:02:55.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:10.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:25.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:40.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:03:55.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:10.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:26.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:41.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:04:56.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:11.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:27.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:42.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:05:57.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:13.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:28.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:43.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:06:59.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:14.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:29.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:45.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:00.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:16.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:31.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:46.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:02.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:17.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:32.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:09:48.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:03.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:19.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:34.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:10:49.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:05.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:20.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:35.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:11:51.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:06.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:22.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:37.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:12:52.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:08.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:23.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:38.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:13:54.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:09.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:24.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:40.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:14:55.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:10.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:26.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:41.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:15:56.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:12.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:27.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:42.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:16:57.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:13.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:28.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:43.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:17:59.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:14.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:29.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:45.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:00.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:15.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:30.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:19:46.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:01.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:16.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:32.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:20:47.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:03.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:18.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:33.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:21:49.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:04.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:19.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:35.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:22:50.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:06.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:21.\n",
            "\n",
            "  Average training loss: 0.91\n",
            "  Training epcoh took: 0:23:21\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/config.json\n",
            "Model weights saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/special_tokens_map.json\n",
            "  Accuracy: 0.96\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:02.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:48.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:34.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:04.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:20.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:35.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:51.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:06.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:21.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:37.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:52.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:07.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:23.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:38.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:53.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:09.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:24.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:40.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:55.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:10.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:26.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:41.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:56.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:12.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:27.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:43.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:58.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:14.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:29.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:44.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:00.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:15.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:31.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:46.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:01.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:17.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:32.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:48.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:03.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:18.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:34.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:49.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:05.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:20.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:36.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:51.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:06.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:22.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:37.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:53.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:08.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:23.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:39.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:54.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:10.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:25.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:40.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:56.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:11.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:27.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:42.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:57.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:13.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:28.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:44.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:59.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:15.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:30.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:45.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:20:01.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:16.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:32.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:47.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:02.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:18.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:33.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:48.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:04.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:19.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:35.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:50.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:05.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:21.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:36.\n",
            "\n",
            "  Average training loss: 0.96\n",
            "  Training epcoh took: 0:23:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:02.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:33.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:48.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:04.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:19.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:35.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:50.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:05.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:21.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:36.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:52.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:07.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:23.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:38.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:54.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:09.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:24.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:40.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:55.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:10.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:26.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:41.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:57.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:12.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:27.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:43.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:58.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:13.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:29.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:44.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:59.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:15.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:30.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:46.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:01.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:16.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:32.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:47.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:02.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:18.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:33.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:48.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:04.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:19.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:34.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:50.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:05.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:20.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:36.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:51.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:07.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:22.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:37.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:53.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:08.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:23.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:39.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:54.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:09.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:25.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:40.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:55.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:11.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:26.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:41.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:57.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:12.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:27.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:43.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:58.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:13.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:28.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:44.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:19:59.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:14.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:30.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:45.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:00.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:15.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:31.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:46.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:01.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:17.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:32.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:47.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:02.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:18.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:33.\n",
            "\n",
            "  Average training loss: 0.89\n",
            "  Training epcoh took: 0:23:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:13\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:47.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:33.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:04.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:20.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:35.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:50.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:06.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:21.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:36.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:52.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:07.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:23.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:38.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:53.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:09.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:24.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:40.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:55.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:10.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:26.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:41.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:57.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:12.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:27.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:43.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:58.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:14.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:29.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:44.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:00.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:15.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:31.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:46.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:01.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:17.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:32.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:48.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:03.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:19.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:34.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:49.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:05.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:20.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:36.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:51.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:06.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:22.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:37.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:53.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:08.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:24.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:39.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:54.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:10.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:25.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:41.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:56.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:11.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:27.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:42.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:58.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:13.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:29.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:44.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:59.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:15.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:30.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:46.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:20:01.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:17.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:32.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:47.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:03.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:18.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:34.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:49.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:05.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:20.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:35.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:51.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:06.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:22.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:37.\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:23:37\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/config.json\n",
            "Model weights saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/special_tokens_map.json\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:31\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:02.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:48.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:19.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:34.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:05.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:20.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:36.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:51.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:06.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:22.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:37.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:53.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:08.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:24.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:39.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:54.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:10.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:25.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:41.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:56.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:12.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:27.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:42.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:58.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:13.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:29.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:44.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:59.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:15.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:30.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:46.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:01.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:17.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:32.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:47.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:03.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:18.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:34.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:49.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:05.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:20.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:35.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:51.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:06.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:22.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:37.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:53.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:08.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:23.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:39.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:54.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:10.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:25.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:40.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:56.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:11.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:27.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:42.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:58.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:13.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:28.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:44.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:59.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:15.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:30.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:45.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:19:01.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:16.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:32.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:47.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:20:02.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:18.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:33.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:49.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:04.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:19.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:35.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:50.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:06.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:21.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:36.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:52.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:07.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:22.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:38.\n",
            "\n",
            "  Average training loss: 0.84\n",
            "  Training epcoh took: 0:23:38\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:48.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:34.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:05.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:20.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:35.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:51.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:06.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:22.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:37.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:52.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:08.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:23.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:39.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:54.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:09.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:25.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:40.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:56.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:11.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:26.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:42.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:57.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:13.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:28.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:44.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:59.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:14.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:30.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:45.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:01.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:16.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:31.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:47.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:02.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:17.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:33.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:48.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:04.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:19.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:34.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:50.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:05.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:21.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:36.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:51.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:07.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:22.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:38.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:53.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:08.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:24.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:39.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:55.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:10.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:25.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:41.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:56.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:12.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:27.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:42.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:58.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:13.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:28.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:44.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:59.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:15.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:30.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:45.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:20:01.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:16.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:32.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:47.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:02.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:18.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:33.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:49.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:04.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:19.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:35.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:50.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:06.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:21.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:36.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:23:37\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/config.json\n",
            "Model weights saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/special_tokens_map.json\n",
            "  Accuracy: 0.97\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:48.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:34.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:04.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:20.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:35.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:51.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:06.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:21.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:37.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:52.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:08.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:23.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:38.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:54.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:09.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:25.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:40.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:56.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:11.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:26.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:42.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:57.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:13.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:28.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:43.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:59.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:14.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:29.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:45.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:00.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:16.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:31.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:46.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:02.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:17.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:32.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:48.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:03.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:18.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:34.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:49.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:04.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:20.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:35.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:51.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:06.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:21.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:37.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:52.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:07.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:23.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:38.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:53.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:09.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:24.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:40.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:55.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:10.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:26.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:41.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:56.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:12.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:27.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:43.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:58.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:13.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:29.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:44.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:19:59.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:15.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:30.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:45.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:01.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:16.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:32.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:47.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:02.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:18.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:33.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:48.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:03.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:19.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:34.\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epcoh took: 0:23:35\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/config.json\n",
            "Model weights saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/special_tokens_map.json\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:16.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:47.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:02.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:33.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:48.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:04.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:19.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:34.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:50.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:05.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:20.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:36.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:51.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:06.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:21.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:37.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:52.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:07.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:23.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:38.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:53.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:09.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:24.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:39.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:54.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:10.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:25.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:40.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:56.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:11.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:26.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:42.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:09:57.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:12.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:28.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:43.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:10:58.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:13.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:29.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:44.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:11:59.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:15.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:30.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:45.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:01.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:16.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:32.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:47.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:02.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:17.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:33.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:48.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:04.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:19.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:34.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:49.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:05.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:20.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:35.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:51.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:06.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:21.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:37.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:52.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:07.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:23.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:38.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:53.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:09.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:24.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:39.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:19:55.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:10.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:25.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:41.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:20:56.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:11.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:27.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:42.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:21:58.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:13.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:28.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:44.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:22:59.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:14.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:30.\n",
            "\n",
            "  Average training loss: 0.61\n",
            "  Training epcoh took: 0:23:30\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/config.json\n",
            "Model weights saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/IndicLangMal/Code-Mixed/special_tokens_map.json\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:30\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:47.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:33.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:04.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:19.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:34.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:50.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:05.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:20.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:36.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:51.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:06.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:22.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:37.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:52.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:08.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:23.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:38.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:54.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:09.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:24.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:40.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:55.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:11.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:26.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:41.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:08:57.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:12.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:27.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:43.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:09:58.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:13.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:29.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:44.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:00.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:15.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:30.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:46.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:01.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:16.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:32.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:47.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:03.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:18.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:33.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:49.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:04.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:20.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:35.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:50.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:06.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:21.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:36.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:52.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:07.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:23.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:38.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:16:53.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:09.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:24.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:39.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:17:55.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:10.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:26.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:41.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:18:56.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:12.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:27.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:42.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:19:58.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:13.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:29.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:44.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:20:59.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:15.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:30.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:45.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:01.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:16.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:32.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:47.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:02.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:18.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:33.\n",
            "\n",
            "  Average training loss: 0.58\n",
            "  Training epcoh took: 0:23:33\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of  3,681.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  3,681.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  3,681.    Elapsed: 0:00:46.\n",
            "  Batch   160  of  3,681.    Elapsed: 0:01:01.\n",
            "  Batch   200  of  3,681.    Elapsed: 0:01:17.\n",
            "  Batch   240  of  3,681.    Elapsed: 0:01:32.\n",
            "  Batch   280  of  3,681.    Elapsed: 0:01:47.\n",
            "  Batch   320  of  3,681.    Elapsed: 0:02:03.\n",
            "  Batch   360  of  3,681.    Elapsed: 0:02:18.\n",
            "  Batch   400  of  3,681.    Elapsed: 0:02:33.\n",
            "  Batch   440  of  3,681.    Elapsed: 0:02:49.\n",
            "  Batch   480  of  3,681.    Elapsed: 0:03:04.\n",
            "  Batch   520  of  3,681.    Elapsed: 0:03:20.\n",
            "  Batch   560  of  3,681.    Elapsed: 0:03:35.\n",
            "  Batch   600  of  3,681.    Elapsed: 0:03:50.\n",
            "  Batch   640  of  3,681.    Elapsed: 0:04:06.\n",
            "  Batch   680  of  3,681.    Elapsed: 0:04:21.\n",
            "  Batch   720  of  3,681.    Elapsed: 0:04:37.\n",
            "  Batch   760  of  3,681.    Elapsed: 0:04:52.\n",
            "  Batch   800  of  3,681.    Elapsed: 0:05:08.\n",
            "  Batch   840  of  3,681.    Elapsed: 0:05:23.\n",
            "  Batch   880  of  3,681.    Elapsed: 0:05:39.\n",
            "  Batch   920  of  3,681.    Elapsed: 0:05:54.\n",
            "  Batch   960  of  3,681.    Elapsed: 0:06:10.\n",
            "  Batch 1,000  of  3,681.    Elapsed: 0:06:25.\n",
            "  Batch 1,040  of  3,681.    Elapsed: 0:06:41.\n",
            "  Batch 1,080  of  3,681.    Elapsed: 0:06:56.\n",
            "  Batch 1,120  of  3,681.    Elapsed: 0:07:12.\n",
            "  Batch 1,160  of  3,681.    Elapsed: 0:07:27.\n",
            "  Batch 1,200  of  3,681.    Elapsed: 0:07:43.\n",
            "  Batch 1,240  of  3,681.    Elapsed: 0:07:58.\n",
            "  Batch 1,280  of  3,681.    Elapsed: 0:08:14.\n",
            "  Batch 1,320  of  3,681.    Elapsed: 0:08:29.\n",
            "  Batch 1,360  of  3,681.    Elapsed: 0:08:44.\n",
            "  Batch 1,400  of  3,681.    Elapsed: 0:09:00.\n",
            "  Batch 1,440  of  3,681.    Elapsed: 0:09:15.\n",
            "  Batch 1,480  of  3,681.    Elapsed: 0:09:31.\n",
            "  Batch 1,520  of  3,681.    Elapsed: 0:09:46.\n",
            "  Batch 1,560  of  3,681.    Elapsed: 0:10:02.\n",
            "  Batch 1,600  of  3,681.    Elapsed: 0:10:17.\n",
            "  Batch 1,640  of  3,681.    Elapsed: 0:10:33.\n",
            "  Batch 1,680  of  3,681.    Elapsed: 0:10:48.\n",
            "  Batch 1,720  of  3,681.    Elapsed: 0:11:04.\n",
            "  Batch 1,760  of  3,681.    Elapsed: 0:11:19.\n",
            "  Batch 1,800  of  3,681.    Elapsed: 0:11:35.\n",
            "  Batch 1,840  of  3,681.    Elapsed: 0:11:50.\n",
            "  Batch 1,880  of  3,681.    Elapsed: 0:12:06.\n",
            "  Batch 1,920  of  3,681.    Elapsed: 0:12:21.\n",
            "  Batch 1,960  of  3,681.    Elapsed: 0:12:37.\n",
            "  Batch 2,000  of  3,681.    Elapsed: 0:12:52.\n",
            "  Batch 2,040  of  3,681.    Elapsed: 0:13:08.\n",
            "  Batch 2,080  of  3,681.    Elapsed: 0:13:23.\n",
            "  Batch 2,120  of  3,681.    Elapsed: 0:13:39.\n",
            "  Batch 2,160  of  3,681.    Elapsed: 0:13:54.\n",
            "  Batch 2,200  of  3,681.    Elapsed: 0:14:10.\n",
            "  Batch 2,240  of  3,681.    Elapsed: 0:14:25.\n",
            "  Batch 2,280  of  3,681.    Elapsed: 0:14:40.\n",
            "  Batch 2,320  of  3,681.    Elapsed: 0:14:56.\n",
            "  Batch 2,360  of  3,681.    Elapsed: 0:15:11.\n",
            "  Batch 2,400  of  3,681.    Elapsed: 0:15:27.\n",
            "  Batch 2,440  of  3,681.    Elapsed: 0:15:42.\n",
            "  Batch 2,480  of  3,681.    Elapsed: 0:15:58.\n",
            "  Batch 2,520  of  3,681.    Elapsed: 0:16:13.\n",
            "  Batch 2,560  of  3,681.    Elapsed: 0:16:29.\n",
            "  Batch 2,600  of  3,681.    Elapsed: 0:16:44.\n",
            "  Batch 2,640  of  3,681.    Elapsed: 0:17:00.\n",
            "  Batch 2,680  of  3,681.    Elapsed: 0:17:15.\n",
            "  Batch 2,720  of  3,681.    Elapsed: 0:17:31.\n",
            "  Batch 2,760  of  3,681.    Elapsed: 0:17:46.\n",
            "  Batch 2,800  of  3,681.    Elapsed: 0:18:02.\n",
            "  Batch 2,840  of  3,681.    Elapsed: 0:18:17.\n",
            "  Batch 2,880  of  3,681.    Elapsed: 0:18:33.\n",
            "  Batch 2,920  of  3,681.    Elapsed: 0:18:48.\n",
            "  Batch 2,960  of  3,681.    Elapsed: 0:19:04.\n",
            "  Batch 3,000  of  3,681.    Elapsed: 0:19:19.\n",
            "  Batch 3,040  of  3,681.    Elapsed: 0:19:34.\n",
            "  Batch 3,080  of  3,681.    Elapsed: 0:19:50.\n",
            "  Batch 3,120  of  3,681.    Elapsed: 0:20:05.\n",
            "  Batch 3,160  of  3,681.    Elapsed: 0:20:21.\n",
            "  Batch 3,200  of  3,681.    Elapsed: 0:20:36.\n",
            "  Batch 3,240  of  3,681.    Elapsed: 0:20:52.\n",
            "  Batch 3,280  of  3,681.    Elapsed: 0:21:07.\n",
            "  Batch 3,320  of  3,681.    Elapsed: 0:21:23.\n",
            "  Batch 3,360  of  3,681.    Elapsed: 0:21:38.\n",
            "  Batch 3,400  of  3,681.    Elapsed: 0:21:54.\n",
            "  Batch 3,440  of  3,681.    Elapsed: 0:22:09.\n",
            "  Batch 3,480  of  3,681.    Elapsed: 0:22:25.\n",
            "  Batch 3,520  of  3,681.    Elapsed: 0:22:40.\n",
            "  Batch 3,560  of  3,681.    Elapsed: 0:22:56.\n",
            "  Batch 3,600  of  3,681.    Elapsed: 0:23:11.\n",
            "  Batch 3,640  of  3,681.    Elapsed: 0:23:27.\n",
            "  Batch 3,680  of  3,681.    Elapsed: 0:23:42.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epcoh took: 0:23:42\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.98\n",
            "  Validation took: 0:00:14\n",
            "\n",
            "Training complete!\n",
            "7\n",
            "Figure(1200x600)\n",
            "{'accuracy': 0.9723427331887202, 'mF1Score': 0.7981745488414416, 'f1Score': 0.6106870229007635, 'auc': 0.7497651235342633, 'precision': 0.7692307692307693, 'recall': 0.5063291139240507}\n"
          ]
        }
      ],
      "source": [
        "!python code/StandAloneTrainingMalAugmented.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQeX-MUP4Pmf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
