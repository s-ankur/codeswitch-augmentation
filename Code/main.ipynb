{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "BKOA3DbEfHaq"
      },
      "source": [
        ">[Installation and Setup](#scrollTo=exY3is1ILr_F)\n",
        "\n",
        ">[Data Loading and Validation](#scrollTo=P7mSZGr-Ln8U)\n",
        "\n",
        ">[Statistics](#scrollTo=D1VzHuOKUkYU)\n",
        "\n",
        ">[Create Merged Datasets](#scrollTo=5-QvhUDFdAxu)\n",
        "\n",
        ">[Experiments](#scrollTo=SIs5sRIzc8Z3)\n",
        "\n",
        ">>[CK](#scrollTo=tPLqugwWdKMk)\n",
        "\n",
        ">>[CK Character Augmentation](#scrollTo=vb51VkkJeHzq)\n",
        "\n",
        ">>[CK_E](#scrollTo=mDn8U6aUdOlT)\n",
        "\n",
        ">>[CK_K](#scrollTo=L6ecz2UVdbki)\n",
        "\n",
        ">>[CK_E_K](#scrollTo=Qp74868Ac6sj)\n",
        "\n",
        ">>[CK Probability Augmentation](#scrollTo=6GtBwpWzeZnp)\n",
        "\n",
        ">>[CK Word Alignment Augmentation](#scrollTo=RXZO8ymieo_J)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exY3is1ILr_F"
      },
      "source": [
        "#Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oOSwi5nStW",
        "outputId": "adadfb2c-e65c-4281-d220-01b393924b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1pyPDXXgFYu_gD-IDMCmfmchXQboVGhNs/CS769\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/CS769"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K-Qea2jBQD9",
        "outputId": "bbf0c5e8-ec4c-49df-c84b-c2dec13ed78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7mSZGr-Ln8U"
      },
      "source": [
        "#Data Loading and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGCBtR5WNl3c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_ck_train =  pd.read_csv(\"original/ck_train.csv\", encoding='utf-8')\n",
        "df_ck_test =  pd.read_csv(\"original/ck_test.csv\", encoding='utf-8')\n",
        "df_ck_dev =  pd.read_csv(\"original/ck_dev.csv\", encoding='utf-8')\n",
        "df_k_train = pd.read_csv(\"translated/k_train.csv\", encoding='utf-8')\n",
        "df_e_train = pd.read_csv(\"translated/e_train.csv\", encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXKWDLWt9ttM"
      },
      "outputs": [],
      "source": [
        "def validate(df):\n",
        "  assert set(df['label'])=={\"Offensive\",\"Not_offensive\"}\n",
        "  assert df.isna().any().any()==False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kWyj7_dGp8K",
        "outputId": "fa007eb2-e0bc-4648-94e7-e94b2993e39a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4695\n"
          ]
        }
      ],
      "source": [
        "validate(df_ck_train)\n",
        "validate(df_ck_test)\n",
        "validate(df_ck_dev)\n",
        "validate(df_k_train)\n",
        "validate(df_e_train)\n",
        "assert len(df_k_train)==len(df_e_train)==len(df_ck_train)\n",
        "print(len(df_k_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1VzHuOKUkYU"
      },
      "source": [
        "#Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4vFw8g79nGs"
      },
      "outputs": [],
      "source": [
        "def balance_stats(df):\n",
        "  dupes=df['text'].value_counts()\n",
        "  labels=df['label'].value_counts()\n",
        "  return pd.Series(dict(\n",
        "      n_duplicates=dupes[dupes>1].sum()-len(dupes[dupes>1]),\n",
        "      offensive=labels['Offensive'],\n",
        "      not_offensive=labels['Not_offensive'],\n",
        "      total=labels.sum()\n",
        "  ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1PFaCopI6JI",
        "outputId": "ace14474-a714-44ad-86b9-8b2641be7ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          n_duplicates  offensive  not_offensive  total\n",
            "ck_train           174       1151           3544   4695\n",
            "ck_test              5        166            427    593\n",
            "ck_dev              14        160            426    586\n",
            "         n_duplicates  offensive  not_offensive  total\n",
            "k_train           216       1151           3544   4695\n",
            "e_train           209       1151           3544   4695\n"
          ]
        }
      ],
      "source": [
        "stats_original=pd.DataFrame(dict(\n",
        "    ck_train=balance_stats(df_ck_train),\n",
        "    ck_test=balance_stats(df_ck_test),\n",
        "    ck_dev=balance_stats(df_ck_dev),\n",
        ")).transpose()\n",
        "stats_translated=pd.DataFrame(dict(\n",
        "    k_train=balance_stats(df_k_train),\n",
        "    e_train=balance_stats(df_e_train),\n",
        ")).transpose()\n",
        "print(stats_original)\n",
        "stats_original.to_csv('stats/stats_original.csv')\n",
        "print(stats_translated)\n",
        "stats_translated.to_csv('stats/stats_translated.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faEX03jWLiYe"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import unicodedata\n",
        "  \n",
        "def dominance_stats(df):\n",
        "\n",
        "  unique_words = set()\n",
        "  for sentence in df['text']:\n",
        "      words = sentence.split()\n",
        "      unique_words.update(words)\n",
        "  unique_words = set(word for word in unique_words if not (all(char in string.punctuation for char in word) or word.isdigit()))\n",
        "  \n",
        "  def is_english_word(word):\n",
        "      try:\n",
        "        for char in word:\n",
        "            if char.isalpha() or char.isspace():\n",
        "                continue\n",
        "            if 'Emoji' in unicodedata.name(char):\n",
        "                continue\n",
        "            if char in string.punctuation:\n",
        "                continue\n",
        "            return False\n",
        "      except ValueError:\n",
        "        return False\n",
        "      return True\n",
        "  english_words = set(filter(is_english_word, unique_words))\n",
        "  return pd.Series(dict(\n",
        "      e_dominance=len(english_words)/len(unique_words),\n",
        "  ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdxYt7ZfzrBT"
      },
      "source": [
        "dominance_e  + dominance_k = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcORFiVH4JgK",
        "outputId": "edaec9d2-3862-48d4-d88b-b24f8669ccaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          e_dominance\n",
            "ck_train     0.628155\n",
            "ck_test      0.621958\n",
            "ck_dev       0.671467\n",
            "         e_dominance\n",
            "k_train     0.256410\n",
            "e_train     0.974296\n"
          ]
        }
      ],
      "source": [
        "dominance_stats_original=pd.DataFrame(dict(\n",
        "    ck_train=dominance_stats(df_ck_train),\n",
        "    ck_test=dominance_stats(df_ck_test),\n",
        "    ck_dev=dominance_stats(df_ck_dev),\n",
        ")).transpose()\n",
        "dominance_stats_translated=pd.DataFrame(dict(\n",
        "    k_train=dominance_stats(df_k_train),\n",
        "    e_train=dominance_stats(df_e_train),\n",
        ")).transpose()\n",
        "print(dominance_stats_original)\n",
        "dominance_stats_original.to_csv('stats/dominance_stats_original.csv')\n",
        "print(dominance_stats_translated)\n",
        "dominance_stats_translated.to_csv('stats/dominance_stats_translated.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-QvhUDFdAxu"
      },
      "source": [
        "#Create Merged Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtlqDjGMOLlG"
      },
      "outputs": [],
      "source": [
        "df_ck_e_k_train=pd.concat([df_ck_train,df_k_train,df_e_train])\n",
        "df_ck_k_train=pd.concat([df_ck_train,df_k_train])\n",
        "df_ck_e_train=pd.concat([df_ck_train,df_e_train])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB5uHoKUOqpA"
      },
      "outputs": [],
      "source": [
        "df_ck_e_k_train.to_csv('translated/ck_e_k_train.csv', index=False)\n",
        "df_ck_k_train.to_csv('translated/ck_k_train.csv', index=False)\n",
        "df_ck_e_train.to_csv('translated/ck_e_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hw9I84MgN7g"
      },
      "source": [
        "#Alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD6AXjaegRr3",
        "outputId": "d15a7ba6-fd8c-4924-f5c4-243b6f206c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "Cloning into 'fast_align'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 213 (delta 2), reused 4 (delta 2), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (213/213), 70.68 KiB | 2.14 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:2 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 2.8.12 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/drive/.shortcut-targets-by-id/1pyPDXXgFYu_gD-IDMCmfmchXQboVGhNs/CS769_Script/fast_align/build\n",
            "[-16%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/fast_align.cc.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/ttables.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX executable fast_align\u001b[0m\n",
            "[ 16%] Built target fast_align\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/alignment_io.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/atools.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable atools\u001b[0m\n",
            "[ 66%] Built target atools\n"
          ]
        }
      ],
      "source": [
        "# Install fast_align using apt-get\n",
        "! sudo apt-get install -y cmake\n",
        "! rm -rf fast_align\n",
        "! git clone https://github.com/clab/fast_align.git\n",
        "! cd fast_align && mkdir build && cd build && cmake .. && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD6LHEfPg1mw",
        "outputId": "8dc9370e-09e4-402a-becd-6c0361512a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "aligned_text_k2e = []\n",
        "aligned_text_e2k = []\n",
        "for k_sentence, e_sentence in zip(df_k_train[\"text\"],df_e_train[\"text\"]):\n",
        "  k_sentence=' '.join(word_tokenize(k_sentence))\n",
        "  e_sentence=' '.join(word_tokenize(e_sentence))\n",
        "  aligned_text_k2e.append(k_sentence + ' ||| ' + e_sentence)\n",
        "  aligned_text_e2k.append(e_sentence + ' ||| ' + k_sentence)\n",
        "\n",
        "with open('aligned/aligned_text_k2e.txt', 'w') as f:\n",
        "    f.write('\\n'.join(aligned_text_k2e))\n",
        "with open('aligned/aligned_text_e2k.txt', 'w') as f:\n",
        "    f.write('\\n'.join(aligned_text_e2k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br5ce_bA_DoB",
        "outputId": "369d8272-865a-4bdd-d079-2bae7cd284bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "INITIAL PASS \n",
            "....\n",
            "expected target length = source length * 1.18822\n",
            "ITERATION 1\n",
            "....\n",
            "  log_e likelihood: -1.09972e+06\n",
            "  log_2 likelihood: -1.58656e+06\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.186238\n",
            "       size counts: 542\n",
            "ITERATION 2\n",
            "....\n",
            "  log_e likelihood: -301491\n",
            "  log_2 likelihood: -434960\n",
            "     cross entropy: 8.19643\n",
            "        perplexity: 293.339\n",
            "      posterior p0: 0.0699176\n",
            " posterior al-feat: -0.131313\n",
            "       size counts: 542\n",
            "  1  model al-feat: -0.146081 (tension=4)\n",
            "  2  model al-feat: -0.141315 (tension=4.29536)\n",
            "  3  model al-feat: -0.138425 (tension=4.49539)\n",
            "  4  model al-feat: -0.136546 (tension=4.63764)\n",
            "  5  model al-feat: -0.13526 (tension=4.74229)\n",
            "  6  model al-feat: -0.134346 (tension=4.82123)\n",
            "  7  model al-feat: -0.133677 (tension=4.8819)\n",
            "  8  model al-feat: -0.133177 (tension=4.92919)\n",
            "     final tension: 4.96647\n",
            "ITERATION 3\n",
            "....\n",
            "  log_e likelihood: -181068\n",
            "  log_2 likelihood: -261226\n",
            "     cross entropy: 4.92257\n",
            "        perplexity: 30.3278\n",
            "      posterior p0: 0.0543052\n",
            " posterior al-feat: -0.126704\n",
            "       size counts: 542\n",
            "  1  model al-feat: -0.132795 (tension=4.96647)\n",
            "  2  model al-feat: -0.13163 (tension=5.0883)\n",
            "  3  model al-feat: -0.130782 (tension=5.18683)\n",
            "  4  model al-feat: -0.130147 (tension=5.2684)\n",
            "  5  model al-feat: -0.129659 (tension=5.33726)\n",
            "  6  model al-feat: -0.129276 (tension=5.39637)\n",
            "  7  model al-feat: -0.128971 (tension=5.44782)\n",
            "  8  model al-feat: -0.128725 (tension=5.49317)\n",
            "     final tension: 5.53359\n",
            "ITERATION 4\n",
            "....\n",
            "  log_e likelihood: -165523\n",
            "  log_2 likelihood: -238800\n",
            "     cross entropy: 4.49997\n",
            "        perplexity: 22.6269\n",
            "      posterior p0: 0.0544304\n",
            " posterior al-feat: -0.124474\n",
            "       size counts: 542\n",
            "  1  model al-feat: -0.128523 (tension=5.53359)\n",
            "  2  model al-feat: -0.12817 (tension=5.61456)\n",
            "  3  model al-feat: -0.127911 (tension=5.68848)\n",
            "  4  model al-feat: -0.127725 (tension=5.7572)\n",
            "  5  model al-feat: -0.127599 (tension=5.82221)\n",
            "  6  model al-feat: -0.127527 (tension=5.88471)\n",
            "  7  model al-feat: -0.127504 (tension=5.94576)\n",
            "  8  model al-feat: -0.127528 (tension=6.00635)\n",
            "     final tension: 6.06742\n",
            "ITERATION 5 (FINAL)\n",
            "....\n",
            "  log_e likelihood: -160820\n",
            "  log_2 likelihood: -232014\n",
            "     cross entropy: 4.37209\n",
            "        perplexity: 20.7077\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 542\n",
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "INITIAL PASS \n",
            "....\n",
            "expected target length = source length * 0.918191\n",
            "ITERATION 1\n",
            "....\n",
            "  log_e likelihood: -936401\n",
            "  log_2 likelihood: -1.35094e+06\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.191344\n",
            "       size counts: 542\n",
            "ITERATION 2\n",
            "....\n",
            "  log_e likelihood: -312757\n",
            "  log_2 likelihood: -451212\n",
            "     cross entropy: 9.98567\n",
            "        perplexity: 1013.88\n",
            "      posterior p0: 0.0382927\n",
            " posterior al-feat: -0.14238\n",
            "       size counts: 542\n",
            "  1  model al-feat: -0.19841 (tension=4)\n",
            "  2  model al-feat: -0.168245 (tension=5.12061)\n",
            "  3  model al-feat: -0.156694 (tension=5.63792)\n",
            "  4  model al-feat: -0.150854 (tension=5.92421)\n",
            "  5  model al-feat: -0.147567 (tension=6.09369)\n",
            "  6  model al-feat: -0.145615 (tension=6.19744)\n",
            "  7  model al-feat: -0.14442 (tension=6.26214)\n",
            "  8  model al-feat: -0.143675 (tension=6.30294)\n",
            "     final tension: 6.32884\n",
            "ITERATION 3\n",
            "....\n",
            "  log_e likelihood: -170013\n",
            "  log_2 likelihood: -245277\n",
            "     cross entropy: 5.42816\n",
            "        perplexity: 43.0564\n",
            "      posterior p0: 0.0234848\n",
            " posterior al-feat: -0.135904\n",
            "       size counts: 542\n",
            "  1  model al-feat: -0.143205 (tension=6.32884)\n",
            "  2  model al-feat: -0.140607 (tension=6.47486)\n",
            "  3  model al-feat: -0.138977 (tension=6.56891)\n",
            "  4  model al-feat: -0.137929 (tension=6.63036)\n",
            "  5  model al-feat: -0.137246 (tension=6.67086)\n",
            "  6  model al-feat: -0.136797 (tension=6.6977)\n",
            "  7  model al-feat: -0.1365 (tension=6.71556)\n",
            "  8  model al-feat: -0.136302 (tension=6.72746)\n",
            "     final tension: 6.73541\n",
            "ITERATION 4\n",
            "....\n",
            "  log_e likelihood: -157481\n",
            "  log_2 likelihood: -227197\n",
            "     cross entropy: 5.02803\n",
            "        perplexity: 32.6278\n",
            "      posterior p0: 0.0212919\n",
            " posterior al-feat: -0.135336\n",
            "       size counts: 542\n",
            "  1  model al-feat: -0.13617 (tension=6.73541)\n",
            "  2  model al-feat: -0.135894 (tension=6.7521)\n",
            "  3  model al-feat: -0.13571 (tension=6.76326)\n",
            "  4  model al-feat: -0.135587 (tension=6.77075)\n",
            "  5  model al-feat: -0.135505 (tension=6.77578)\n",
            "  6  model al-feat: -0.135449 (tension=6.77915)\n",
            "  7  model al-feat: -0.135412 (tension=6.78142)\n",
            "  8  model al-feat: -0.135387 (tension=6.78294)\n",
            "     final tension: 6.78396\n",
            "ITERATION 5 (FINAL)\n",
            "....\n",
            "  log_e likelihood: -154935\n",
            "  log_2 likelihood: -223524\n",
            "     cross entropy: 4.94675\n",
            "        perplexity: 30.8403\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 542\n"
          ]
        }
      ],
      "source": [
        "!fast_align/build/fast_align -i aligned/aligned_text_k2e.txt -d -o -v > aligned/k2e_align.txt\n",
        "!fast_align/build/fast_align -i aligned/aligned_text_e2k.txt -d -o -v > aligned/e2k_align.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdK2lKXdhzN1"
      },
      "outputs": [],
      "source": [
        "def align_dict(direction):\n",
        "  with open(f'aligned/{direction}_align.txt', 'r', encoding='utf-8') as f:\n",
        "    alignments = [line.strip().split() for line in f]\n",
        "  dictionary = list()\n",
        "  for sentence in alignments:\n",
        "    temp=dict()\n",
        "    for pair in sentence:\n",
        "      i, j = pair.split(\"-\")\n",
        "      i, j = int(i), int(j)\n",
        "      temp[i]=j\n",
        "    dictionary.append(temp)  \n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgPQmqdGiCWQ"
      },
      "outputs": [],
      "source": [
        "dictionary_k2e=align_dict('k2e')\n",
        "dictionary_e2k=align_dict('e2k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlq9PvYgCjuA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "dominance_e=0.628155\n",
        "def augment_aligned(df_l1_train,df_l2_train,dictionary,dominance):\n",
        "  text=  [] \n",
        "  labels = []\n",
        "  for l1_sentence,l2_sentence,label,dict_line  in zip(df_l1_train[\"text\"],df_l2_train[\"text\"],df_l1_train[\"label\"],dictionary):\n",
        "    sent=[]\n",
        "    l1_sentence=word_tokenize(l1_sentence)\n",
        "    l2_sentence=word_tokenize(l2_sentence)\n",
        "    for index,word in enumerate(l1_sentence):\n",
        "      if index in dict_line and random.random()<dominance:\n",
        "        sent.append(l2_sentence[dict_line[index]])\n",
        "      else:\n",
        "        sent.append(word)\n",
        "    labels.append(label)\n",
        "    text.append(' '.join(sent))\n",
        "  return pd.DataFrame(dict(text=text,label=labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13pu4CHkc263",
        "outputId": "99ff9e83-1743-4d8a-bde6-5e34d6eb0a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18780 18780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-7df4e4581e83>:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_ckwk2e_train = df_ckwk2e_train.append(df_ck_train)\n",
            "<ipython-input-17-7df4e4581e83>:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df_ckwe2k_train = df_ckwe2k_train.append(df_ck_train)\n"
          ]
        }
      ],
      "source": [
        "df_ckwk2e_train=[]\n",
        "df_ckwe2k_train=[]\n",
        "for i in range(3):\n",
        "  df_ckwk2e_train.append(augment_aligned(df_k_train,df_e_train, dictionary_k2e, dominance_e))\n",
        "  df_ckwe2k_train.append(augment_aligned(df_e_train,df_k_train, dictionary_e2k, 1-dominance_e))\n",
        "df_ckwk2e_train=pd.concat(df_ckwk2e_train)\n",
        "df_ckwe2k_train=pd.concat(df_ckwe2k_train)\n",
        "df_ckwk2e_train = df_ckwk2e_train.append(df_ck_train)\n",
        "df_ckwe2k_train = df_ckwe2k_train.append(df_ck_train)\n",
        "print(len(df_ckwk2e_train), len(df_ckwe2k_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ck_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUyJHT4GwHAv",
        "outputId": "39c9ed6a-3051-49ca-e59c-0912942cae82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text          label\n",
            "0  Tik tok alli jagala madtidralla adra baggenu o...  Not_offensive\n",
            "1                          Movie rerelease madi plss  Not_offensive\n",
            "2  Amazon prime alli bittidira....yella manele no...  Not_offensive\n",
            "3  Guru sure news nanu tik tok dawn lod madeda ya...  Not_offensive\n",
            "4  ಸುದೀಪ್ ಸರ್ ಅಂಡ್ ದರ್ಶನ್ ಸರ್ ಅವರಿಗೆ ಇರೋ ಫ್ಯಾನ್ಸ್...  Not_offensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ckwk2e_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyy_tPDrwTOO",
        "outputId": "099e516f-4892-4b53-bfd1-92d78de6eb69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text          label\n",
            "0  ತಿಕ್ Tok ಅಲ್ಲಿ ಜಗಳ ಮಾಡ್ತಿದ್ರಲ್ಲ ಅದ್ರ ಬಗ್ಗೆನೂ ಒ...  Not_offensive\n",
            "1                         Movie ರೇರೆಳೆಯಾಸೆ ಮಾಡಿ plss  Not_offensive\n",
            "2  Amazon ಪ್ರೈಮ್ ಅಲ್ಲಿ Prime .... is there ನೋಡ್ತಾ...  Not_offensive\n",
            "3  ಗುರು Nivs ನಿವ್ಸ್ ನಾನು ತಿಕ್ Tok ಡಾನ್ನ ಲೋಡ್ ಮಾಡಿ...  Not_offensive\n",
            "4  Sudeep sir and ದರ್ಶನ್ ಸರ್ ಅವರಿಗೆ ಇರೋ ಫ್ಯಾನ್ಸ್ ...  Not_offensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_ckwe2k_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHu_GafKwTst",
        "outputId": "1d7c9991-bc6d-4f85-fd83-cb94f734c5ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text          label\n",
            "0  Did you make ಒಂದು ವಿಡಿಯೋ about Tik Tok and bec...  Not_offensive\n",
            "1                         Movie rerelease madi ಪ್ಲಸ್  Not_offensive\n",
            "2                        Amazon .... is ನೋಡ್ತಾರೆ ...  Not_offensive\n",
            "3  ಗುರು Sure Nivs When I ಡಾನ್ನ Tik Tok ಮಾಡಿದ ಯಾವಾ...  Not_offensive\n",
            "4  ಸುದೀಪ್ ಸರ್ and ದರ್ಶನ್ ಸರ್ have Ero ಫ್ಯಾನ್ಸ್ fo...  Not_offensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_k_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01JFFL3C1FEu",
        "outputId": "038ef787-7d2b-4a9a-8149-25722c1ff57a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text          label\n",
            "0  ತಿಕ್ ಟಾಕ್ ಅಲ್ಲಿ ಜಗಳ ಮಾಡ್ತಿದ್ರಲ್ಲ ಅದ್ರ ಬಗ್ಗೆನೂ ...  Not_offensive\n",
            "1                         ಮೂವಿ ರೇರೆಳೆಯಾಸೆ ಮಾಡಿ ಪ್ಲಸ್  Not_offensive\n",
            "2  ಅಮೆಜಾನ್ ಪ್ರೈಮ್ ಅಲ್ಲಿ ಬಿತ್ತಿದಿರಾ....ಎಲ್ಲ ಮನೇಲೆ ...  Not_offensive\n",
            "3  ಗುರು ಸುರೆ ನಿವ್ಸ್ ನಾನು ತಿಕ್ ತೊಕ್ ಡಾನ್ನ ಲೋಡ್ ಮಾಡ...  Not_offensive\n",
            "4  ಸುದೀಪ್ ಸರ್ ಅಂಡ್ ದರ್ಶನ್ ಸರ್ ಅವರಿಗೆ ಇರೋ ಫ್ಯಾನ್ಸ್...  Not_offensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_e_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wQlZrKVwblO",
        "outputId": "93b260f6-f916-4a84-aa5e-79fcc5575ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text          label\n",
            "0  Did you make a video about Tik Tok and become ...  Not_offensive\n",
            "1                          Movie rerelease madi plss  Not_offensive\n",
            "2                           Amazon Prime is there...  Not_offensive\n",
            "3  Guru Sure Nivs When I Loaded Tik Tok Don When ...  Not_offensive\n",
            "4  Sudeep sir and Darshan sir have Ero fans follo...  Not_offensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dx0-jpIdTjT"
      },
      "outputs": [],
      "source": [
        "df_ckwk2e_train.to_csv('augmented/ckwk2e_train.csv', index=False)\n",
        "df_ckwe2k_train.to_csv('augmented/ckwe2e_train.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SV91dxjbSGp",
        "outputId": "575e29c3-1393-4a1a-8e60-552145584dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e_dominance    0.488936\n",
            "dtype: float64\n",
            "e_dominance    0.581811\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(dominance_stats(df_ckwk2e_train))\n",
        "print(dominance_stats(df_ckwe2k_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKDzQwOBYkUs"
      },
      "outputs": [],
      "source": [
        "df_ckwk2e_train = []\n",
        "e_dominance=0.628155\n",
        "for k_sentence,e_sentence,label, e2k in zip(df_k_train[\"text\"],df_e_train[\"text\"],df_k_train[\"label\"],dictionary_e2k):\n",
        "  sent=[]\n",
        "  k_sentence=word_tokenize(k_sentence)\n",
        "  e_sentence=word_tokenize(e_sentence)\n",
        "  for index,word in enumerate(e_sentence):\n",
        "    if random.random()>e_dominance:\n",
        "      sent.append(e_sentence[e2k[index]])\n",
        "    else:\n",
        "      sent.append(word)\n",
        "  df_ckwk2e_train.append(' '.join(sent))\n",
        "df_ckwk2e_train=pd.DataFrame(df_ckwk2e_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LnjcYRlw5B6"
      },
      "source": [
        "\n",
        "\n",
        "Plan for CKalignK2E\n",
        "\n",
        "```\n",
        "Start from a sentence from K-> (k, k_i)\n",
        "for each word j in k:\n",
        "  if roll dice with p (dominance_e):\n",
        "    replace word with e[dictionaryK2E[k_i][j]]\n",
        "\n",
        "dominance_e(CKalign)==dominance_e(CK)\n",
        "```\n",
        "\n",
        " CKalignE2K\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RhUoQ3ax0OF"
      },
      "source": [
        "CKalignCK2CK\n",
        "\n",
        "```\n",
        "Start from a sentence in CK -> (ck, ck_i)\n",
        "for each word j in ck:\n",
        "  if word is english:\n",
        "    if roll dice p=dominence_k:\n",
        "      replace word with k[dictionaryE2K[ck_i][j]]\n",
        "  same fr k\n",
        "\n",
        "dominance_e(CKalign)==1-dominance_e(CK)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIs5sRIzc8Z3"
      },
      "source": [
        "#Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-U7N1aXdYjL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import *\n",
        "def show_metrics(folder_name):\n",
        "      df=pd.read_csv(f'models/{folder_name}/test_pred.csv')\n",
        "      y_true, y_pred=list(df['label']), list(df['pred'])\n",
        "      accuracy = accuracy_score(y_true, y_pred)\n",
        "      mf1Score = f1_score(y_true, y_pred, average='macro')\n",
        "      f1Score  = f1_score(y_true, y_pred)\n",
        "      fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "      area_under_c = auc(fpr, tpr)\n",
        "      recallScore = recall_score(y_true, y_pred)\n",
        "      precisionScore = precision_score(y_true, y_pred)\n",
        "      return {\"accuracy\": accuracy, 'mF1Score': mf1Score, 'f1Score': f1Score, 'auc': area_under_c,'precision': precisionScore, 'recall': recallScore}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPLqugwWdKMk"
      },
      "source": [
        "## CK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOpuENgpdJtU"
      },
      "outputs": [],
      "source": [
        "!python train_generic.py original/ck_train.csv original/ck_dev.csv original/ck_test.csv  models/ck/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfFOD3nr1Dqp"
      },
      "outputs": [],
      "source": [
        "show_metrics('ck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb51VkkJeHzq"
      },
      "source": [
        "## CK Character Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KeovJzqdeoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d5cd22-71c1-45bc-8555-ea12f924b35e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-30 19:45:38.437938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 19:45:43.976801: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Downloading (…)solve/main/vocab.txt: 100% 872k/872k [00:00<00:00, 6.84MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 199kB/s]\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer_config.json\n",
            "Downloading (…)lve/main/config.json: 100% 625/625 [00:00<00:00, 4.51MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "100% 18780/18780 [00:06<00:00, 2868.96it/s]\n",
            "100% 18780/18780 [00:00<00:00, 61008.25it/s]\n",
            "100% 18780/18780 [00:01<00:00, 18002.28it/s]\n",
            "100% 586/586 [00:00<00:00, 3783.86it/s]\n",
            "100% 586/586 [00:00<00:00, 68578.74it/s]\n",
            "100% 586/586 [00:00<00:00, 20899.30it/s]\n",
            "100% 593/593 [00:00<00:00, 3813.52it/s]\n",
            "100% 593/593 [00:00<00:00, 59601.31it/s]\n",
            "100% 593/593 [00:00<00:00, 19932.22it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 672M/672M [00:02<00:00, 286MB/s]\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing SC_weighted_BERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing SC_weighted_BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SC_weighted_BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SC_weighted_BERT were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['loss_fct.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:17.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:30.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:11.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:36.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:31.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:14.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/cka/config.json\n",
            "Model weights saved in models/cka/pytorch_model.bin\n",
            "tokenizer config file saved in models/cka/tokenizer_config.json\n",
            "Special tokens file saved in models/cka/special_tokens_map.json\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:11\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:36.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:05.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:48.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:31.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:00.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:14.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:43.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:14.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:14.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:36.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:31.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:14.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:43.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:36.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:14.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:36.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:19.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:02.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:45.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:28.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:57.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:07:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/cka/config.json\n",
            "Model weights saved in models/cka/pytorch_model.bin\n",
            "tokenizer config file saved in models/cka/tokenizer_config.json\n",
            "Special tokens file saved in models/cka/special_tokens_map.json\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:49.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:32.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:15.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.79\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:49.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:49.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:32.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:15.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:07.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:50.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:33.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:49.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:32.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:15.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:58.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:41.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:57.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:26.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:40.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:55.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:09.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:23.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:38.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:52.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:06.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:21.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:35.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:49.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:04.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:18.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:32.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:47.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:01.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:16.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:30.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:44.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:05:59.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:13.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:27.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:42.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:56.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:07:01\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n",
            "8\n",
            "{'accuracy': 0.7976391231028668, 'mF1Score': 0.7471431008016374, 'f1Score': 0.6341463414634146, 'auc': 0.7453373211816822, 'precision': 0.6419753086419753, 'recall': 0.6265060240963856}\n"
          ]
        }
      ],
      "source": [
        "!python train_generic.py augmented/cka_train.csv original/ck_dev.csv original/ck_test.csv  models/cka/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRfSVyWieSzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a9f2e9-3acc-4fb1-9c54-3759f70633b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7976391231028668,\n",
              " 'mF1Score': 0.7471431008016374,\n",
              " 'f1Score': 0.6341463414634146,\n",
              " 'auc': 0.7453373211816822,\n",
              " 'precision': 0.6419753086419753,\n",
              " 'recall': 0.6265060240963856}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "show_metrics('cka')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDn8U6aUdOlT"
      },
      "source": [
        "## CK_E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJrFlehxdO-y"
      },
      "outputs": [],
      "source": [
        "!python train_generic.py translated/ck_e_train.csv original/ck_dev.csv original/ck_test.csv  models/ck_e/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ3X9MvIeSLZ"
      },
      "outputs": [],
      "source": [
        "show_metrics('ck_e')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ecz2UVdbki"
      },
      "source": [
        "##CK_K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Q9tJNNdVEu"
      },
      "outputs": [],
      "source": [
        "!python train_generic.py translated/ck_k_train.csv original/ck_dev.csv original/ck_test.csv  models/ck_k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Mgik4beQZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83d3d20-10f1-41f0-d076-6ffcd80e68d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8246205733558178,\n",
              " 'mF1Score': 0.7689910401246591,\n",
              " 'f1Score': 0.6556291390728476,\n",
              " 'auc': 0.754867244152253,\n",
              " 'precision': 0.7279411764705882,\n",
              " 'recall': 0.5963855421686747}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "show_metrics('ck_k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp74868Ac6sj"
      },
      "source": [
        "## CK_E_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf9r9ZV_o2uQ"
      },
      "outputs": [],
      "source": [
        "!python train_generic.py translated/ck_e_k_train.csv  original/ck_dev.csv original/ck_test.csv  models/ck_e_k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhBYCEJieTVq"
      },
      "outputs": [],
      "source": [
        "show_metrics('ck_e_k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZO8ymieo_J"
      },
      "source": [
        "## CK Word Alignment Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf1WldIoeoqx"
      },
      "outputs": [],
      "source": [
        "!python train_generic.py augmented/ckwk2e_train.csv  original/ck_dev.csv original/ck_test.csv  models/ckwk2e/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cj-cN9OewAs",
        "outputId": "84933c41-a259-4312-8615-0b92f74a94db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8161888701517707,\n",
              " 'mF1Score': 0.7594499544109712,\n",
              " 'f1Score': 0.6426229508196721,\n",
              " 'auc': 0.7471713552100675,\n",
              " 'precision': 0.7050359712230215,\n",
              " 'recall': 0.5903614457831325}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "show_metrics('ckwk2e')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaV-Aqmtd6Yf",
        "outputId": "4b7bb110-0d61-4c83-e8ed-a35355bd57b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-28 22:12:08.860304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-28 22:12:19.680355: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Downloading (…)solve/main/vocab.txt: 100% 872k/872k [00:00<00:00, 1.02MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 164kB/s]\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer_config.json\n",
            "Downloading (…)lve/main/config.json: 100% 625/625 [00:00<00:00, 3.71MB/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "100% 18780/18780 [00:04<00:00, 3775.23it/s]\n",
            "100% 18780/18780 [00:00<00:00, 68862.05it/s]\n",
            "100% 18780/18780 [00:00<00:00, 18925.03it/s]\n",
            "100% 586/586 [00:00<00:00, 3552.58it/s]\n",
            "100% 586/586 [00:00<00:00, 61048.21it/s]\n",
            "100% 586/586 [00:00<00:00, 20668.72it/s]\n",
            "100% 593/593 [00:00<00:00, 3628.72it/s]\n",
            "100% 593/593 [00:00<00:00, 62141.72it/s]\n",
            "100% 593/593 [00:00<00:00, 20780.19it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "Downloading pytorch_model.bin: 100% 672M/672M [00:06<00:00, 100MB/s] \n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing SC_weighted_BERT: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing SC_weighted_BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SC_weighted_BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SC_weighted_BERT were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'loss_fct.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:16.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:31.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:45.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:01:00.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:15.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:29.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:44.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:58.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:13.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:27.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:42.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:56.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:11.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:25.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:40.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:54.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:09.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:23.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:38.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:52.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:07.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:21.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:35.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:50.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:04.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:19.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:33.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:48.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:02.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:07:07\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/ckwe2e/config.json\n",
            "Model weights saved in models/ckwe2e/pytorch_model.bin\n",
            "tokenizer config file saved in models/ckwe2e/tokenizer_config.json\n",
            "Special tokens file saved in models/ckwe2e/special_tokens_map.json\n",
            "  Accuracy: 0.76\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:21.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:50.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:19.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:48.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:17.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:46.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/ckwe2e/config.json\n",
            "Model weights saved in models/ckwe2e/pytorch_model.bin\n",
            "tokenizer config file saved in models/ckwe2e/tokenizer_config.json\n",
            "Special tokens file saved in models/ckwe2e/special_tokens_map.json\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:13.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:21.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:50.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/ckwe2e/config.json\n",
            "Model weights saved in models/ckwe2e/pytorch_model.bin\n",
            "tokenizer config file saved in models/ckwe2e/tokenizer_config.json\n",
            "Special tokens file saved in models/ckwe2e/special_tokens_map.json\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:21.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/ckwe2e/config.json\n",
            "Model weights saved in models/ckwe2e/pytorch_model.bin\n",
            "tokenizer config file saved in models/ckwe2e/tokenizer_config.json\n",
            "Special tokens file saved in models/ckwe2e/special_tokens_map.json\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:12\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:21.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:50.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:15.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:44.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:21.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:21.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:31.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:07:00.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:24.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:53.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:22.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:51.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:32.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:01.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "  Batch    40  of  1,174.    Elapsed: 0:00:14.\n",
            "  Batch    80  of  1,174.    Elapsed: 0:00:29.\n",
            "  Batch   120  of  1,174.    Elapsed: 0:00:43.\n",
            "  Batch   160  of  1,174.    Elapsed: 0:00:58.\n",
            "  Batch   200  of  1,174.    Elapsed: 0:01:12.\n",
            "  Batch   240  of  1,174.    Elapsed: 0:01:27.\n",
            "  Batch   280  of  1,174.    Elapsed: 0:01:41.\n",
            "  Batch   320  of  1,174.    Elapsed: 0:01:56.\n",
            "  Batch   360  of  1,174.    Elapsed: 0:02:10.\n",
            "  Batch   400  of  1,174.    Elapsed: 0:02:25.\n",
            "  Batch   440  of  1,174.    Elapsed: 0:02:39.\n",
            "  Batch   480  of  1,174.    Elapsed: 0:02:54.\n",
            "  Batch   520  of  1,174.    Elapsed: 0:03:08.\n",
            "  Batch   560  of  1,174.    Elapsed: 0:03:23.\n",
            "  Batch   600  of  1,174.    Elapsed: 0:03:37.\n",
            "  Batch   640  of  1,174.    Elapsed: 0:03:52.\n",
            "  Batch   680  of  1,174.    Elapsed: 0:04:06.\n",
            "  Batch   720  of  1,174.    Elapsed: 0:04:20.\n",
            "  Batch   760  of  1,174.    Elapsed: 0:04:35.\n",
            "  Batch   800  of  1,174.    Elapsed: 0:04:49.\n",
            "  Batch   840  of  1,174.    Elapsed: 0:05:04.\n",
            "  Batch   880  of  1,174.    Elapsed: 0:05:18.\n",
            "  Batch   920  of  1,174.    Elapsed: 0:05:33.\n",
            "  Batch   960  of  1,174.    Elapsed: 0:05:47.\n",
            "  Batch 1,000  of  1,174.    Elapsed: 0:06:02.\n",
            "  Batch 1,040  of  1,174.    Elapsed: 0:06:16.\n",
            "  Batch 1,080  of  1,174.    Elapsed: 0:06:30.\n",
            "  Batch 1,120  of  1,174.    Elapsed: 0:06:45.\n",
            "  Batch 1,160  of  1,174.    Elapsed: 0:06:59.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:07:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n",
            "6\n",
            "{'accuracy': 0.8010118043844857, 'mF1Score': 0.7592416735480318, 'f1Score': 0.6589595375722543, 'auc': 0.7660901216105641, 'precision': 0.6333333333333333, 'recall': 0.6867469879518072}\n"
          ]
        }
      ],
      "source": [
        "!python train_generic.py augmented/ckwe2e_train.csv  original/ck_dev.csv original/ck_test.csv  models/ckwe2e/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDvOvaJLd_X2",
        "outputId": "2c5ee8e5-d439-4fed-fbd3-a5714504b355"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8010118043844857,\n",
              " 'mF1Score': 0.7592416735480318,\n",
              " 'f1Score': 0.6589595375722543,\n",
              " 'auc': 0.7660901216105641,\n",
              " 'precision': 0.6333333333333333,\n",
              " 'recall': 0.6867469879518072}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "show_metrics('ckwe2e')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CK Translated Test Test and "
      ],
      "metadata": {
        "id": "BpI2vxmiZONa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R models/ck_e models/translatedtest_ck_e\n",
        "!cp -R models/ck_k models/translatedtest_ck_k"
      ],
      "metadata": {
        "id": "frRHnggXb0ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_generic.py translated/ck_k_train.csv  translated/k_dev.csv translated/k_test.csv  models/translatedtest_ck_k/"
      ],
      "metadata": {
        "id": "5aQfjhbHdYFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abfc362-4c94-45ac-a1b6-d78c9796d531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-30 18:46:35.934903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 18:46:54.131820: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "100% 9390/9390 [00:03<00:00, 2995.65it/s]\n",
            "100% 9390/9390 [00:00<00:00, 52736.50it/s]\n",
            "100% 9390/9390 [00:00<00:00, 16812.95it/s]\n",
            "100% 586/586 [00:00<00:00, 3339.06it/s]\n",
            "100% 586/586 [00:00<00:00, 51273.83it/s]\n",
            "100% 586/586 [00:00<00:00, 21263.62it/s]\n",
            "100% 593/593 [00:00<00:00, 3323.20it/s]\n",
            "100% 593/593 [00:00<00:00, 54415.47it/s]\n",
            "100% 593/593 [00:00<00:00, 21378.17it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing SC_weighted_BERT: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing SC_weighted_BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SC_weighted_BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SC_weighted_BERT were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'loss_fct.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of    587.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    587.    Elapsed: 0:00:29.\n",
            "  Batch   120  of    587.    Elapsed: 0:00:42.\n",
            "  Batch   160  of    587.    Elapsed: 0:00:56.\n",
            "  Batch   200  of    587.    Elapsed: 0:01:09.\n",
            "  Batch   240  of    587.    Elapsed: 0:01:23.\n",
            "  Batch   280  of    587.    Elapsed: 0:01:37.\n",
            "  Batch   320  of    587.    Elapsed: 0:01:52.\n",
            "  Batch   360  of    587.    Elapsed: 0:02:06.\n",
            "  Batch   400  of    587.    Elapsed: 0:02:20.\n",
            "  Batch   440  of    587.    Elapsed: 0:02:35.\n",
            "  Batch   480  of    587.    Elapsed: 0:02:49.\n",
            "  Batch   520  of    587.    Elapsed: 0:03:03.\n",
            "  Batch   560  of    587.    Elapsed: 0:03:17.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:03:27\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/translatedtest_ck_k/config.json\n",
            "Model weights saved in models/translatedtest_ck_k/pytorch_model.bin\n",
            "tokenizer config file saved in models/translatedtest_ck_k/tokenizer_config.json\n",
            "Special tokens file saved in models/translatedtest_ck_k/special_tokens_map.json\n",
            "  Accuracy: 0.76\n",
            "  Validation took: 0:00:11\n",
            "\n",
            "Training complete!\n",
            "0\n",
            "{'accuracy': 0.806070826306914, 'mF1Score': 0.7657599791160742, 'f1Score': 0.6685878962536024, 'auc': 0.7732851781834599, 'precision': 0.6408839779005525, 'recall': 0.6987951807228916}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_metrics('translatedtest_ck_k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FkmzdefQTX",
        "outputId": "633825f7-3961-4c89-820b-95320b987f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.806070826306914,\n",
              " 'mF1Score': 0.7657599791160742,\n",
              " 'f1Score': 0.6685878962536024,\n",
              " 'auc': 0.7732851781834599,\n",
              " 'precision': 0.6408839779005525,\n",
              " 'recall': 0.6987951807228916}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_generic.py translated/ck_e_train.csv  translated/e_dev.csv translated/e_test.csv  models/translatedtest_ck_e/"
      ],
      "metadata": {
        "id": "mUx8NfwaZRb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80c5e73-258d-4e8d-d58b-8ef6206b38dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-30 18:52:19.370065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 18:52:27.275852: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "100% 9390/9390 [00:02<00:00, 3895.25it/s]\n",
            "100% 9390/9390 [00:00<00:00, 70000.99it/s]\n",
            "100% 9390/9390 [00:00<00:00, 17881.91it/s]\n",
            "100% 586/586 [00:00<00:00, 4082.81it/s]\n",
            "100% 586/586 [00:00<00:00, 80784.29it/s]\n",
            "100% 586/586 [00:00<00:00, 21850.97it/s]\n",
            "100% 593/593 [00:00<00:00, 4108.75it/s]\n",
            "100% 593/593 [00:00<00:00, 84175.66it/s]\n",
            "100% 593/593 [00:00<00:00, 17230.74it/s]\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.28.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing SC_weighted_BERT: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing SC_weighted_BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SC_weighted_BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SC_weighted_BERT were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'loss_fct.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of    587.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    587.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    587.    Elapsed: 0:00:42.\n",
            "  Batch   160  of    587.    Elapsed: 0:00:57.\n",
            "  Batch   200  of    587.    Elapsed: 0:01:11.\n",
            "  Batch   240  of    587.    Elapsed: 0:01:25.\n",
            "  Batch   280  of    587.    Elapsed: 0:01:39.\n",
            "  Batch   320  of    587.    Elapsed: 0:01:53.\n",
            "  Batch   360  of    587.    Elapsed: 0:02:08.\n",
            "  Batch   400  of    587.    Elapsed: 0:02:22.\n",
            "  Batch   440  of    587.    Elapsed: 0:02:36.\n",
            "  Batch   480  of    587.    Elapsed: 0:02:50.\n",
            "  Batch   520  of    587.    Elapsed: 0:03:04.\n",
            "  Batch   560  of    587.    Elapsed: 0:03:18.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:03:28\n",
            "\n",
            "Running Validation...\n",
            "Configuration saved in models/translatedtest_ck_e/config.json\n",
            "Model weights saved in models/translatedtest_ck_e/pytorch_model.bin\n",
            "tokenizer config file saved in models/translatedtest_ck_e/tokenizer_config.json\n",
            "Special tokens file saved in models/translatedtest_ck_e/special_tokens_map.json\n",
            "  Accuracy: 0.75\n",
            "  Validation took: 0:00:11\n",
            "\n",
            "Training complete!\n",
            "0\n",
            "{'accuracy': 0.7672849915682968, 'mF1Score': 0.7313041765169426, 'f1Score': 0.6329787234042553, 'auc': 0.7518763578905787, 'precision': 0.5666666666666667, 'recall': 0.7168674698795181}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_metrics('translatedtest_ck_e')"
      ],
      "metadata": {
        "id": "D4egtsaAlB1N",
        "outputId": "c968ea33-c552-4f30-8584-ec87c2496b7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7672849915682968,\n",
              " 'mF1Score': 0.7313041765169426,\n",
              " 'f1Score': 0.6329787234042553,\n",
              " 'auc': 0.7518763578905787,\n",
              " 'precision': 0.5666666666666667,\n",
              " 'recall': 0.7168674698795181}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2xNYjDalDkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}